{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10083817,"sourceType":"datasetVersion","datasetId":6216891}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**QUE- 6 Implementing Neural Style Transfer**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom torch.optim import Adam, LBFGS\nfrom torch.autograd import Variable\nimport torchvision.models as models\nimport cv2 as cv\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nimport sys\nsys.path.append('/kaggle/input/neural-style-transfer-data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:02.320022Z","iopub.execute_input":"2025-01-12T16:22:02.320358Z","iopub.status.idle":"2025-01-12T16:22:06.717339Z","shell.execute_reply.started":"2025-01-12T16:22:02.320308Z","shell.execute_reply":"2025-01-12T16:22:06.716619Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"Imagenet_mean = [123.675, 116.28, 103.53]\nImagenet_std = [1, 1, 1]\n\ndef load_image(img_path, target_shape=None):\n    if not os.path.exists(img_path):\n        raise Exception(f'Path does not exist: {img_path}')\n    img = cv.imread(img_path)[:, :, ::-1]  \n\n    if target_shape is not None:  \n        if isinstance(target_shape, int) and target_shape != -1:  \n            current_height, current_width = img.shape[:2]\n            new_height = target_shape\n            new_width = int(current_width * (new_height / current_height))\n            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n        else:  \n            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n\n    img = img.astype(np.float32)  \n    img /= 255.0 \n    return img\n\ndef prepare_img(img_path, target_shape, device):\n    img = load_image(img_path, target_shape=target_shape)\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.mul(255)),\n        transforms.Normalize(mean=Imagenet_mean, std=Imagenet_std)\n    ])\n\n    img = transform(img).to(device).unsqueeze(0)\n\n    return img\n\ndef save_image(img, img_path):\n    if len(img.shape) == 2:\n        img = np.stack((img,) * 3, axis=-1)\n    cv.imwrite(img_path, img[:, :, ::-1])  \n\ndef generate_out_name(config):\n    prefix = os.path.basename(config['content_img_name']).split('.')[0] + '_' + os.path.basename(config['style_img_name']).split('.')[0]\n    if 'reconstruct_script' in config:\n        suffix = f'_o_{config[\"optimizer\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}{config[\"img_format\"][1]}'\n    else:\n        suffix = f'_o_{config[\"optimizer\"]}_i_{config[\"init_method\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}_cw_{config[\"content_weight\"]}_sw_{config[\"style_weight\"]}_tv_{config[\"tv_weight\"]}{config[\"img_format\"][1]}'\n    return prefix + suffix\n\ndef save_and_maybe_display(opt_img, dump_path, config, img_id, no_iter, display=False):\n    savefreq = config['savefreq']\n    out_img = opt_img.squeeze(axis=0).to('cpu').detach().numpy()\n    out_img = np.moveaxis(out_img, 0, 2)\n\n    if img_id == no_iter-1 or (savefreq > 0 and img_id % savefreq == 0):\n        img_format = config['img_format']\n        out_name = str(img_id).zfill(img_format[0]) + img_format[1] if savefreq != -1 else generate_out_name(config)\n        dump_img = np.copy(out_img)\n        dump_img += np.array(Imagenet_mean).reshape((1, 1, 3))\n        dump_img = np.clip(dump_img, 0, 255).astype('uint8')\n        cv.imwrite(os.path.join(dump_path, out_name), dump_img[:, :, ::-1])\n\n    if display:\n        plt.imshow(np.uint8(uint8_range(out_img)))\n        plt.show()\n\ndef uint8_range(x):\n    if isinstance(x, np.ndarray):\n        x -= np.min(x)\n        x /= np.max(x)\n        x *= 255\n        return x\n    else:\n        raise ValueError(f'Expected numpy array got {type(x)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.718452Z","iopub.execute_input":"2025-01-12T16:22:06.718785Z","iopub.status.idle":"2025-01-12T16:22:06.731423Z","shell.execute_reply.started":"2025-01-12T16:22:06.718758Z","shell.execute_reply":"2025-01-12T16:22:06.730603Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Vgg19(torch.nn.Module):\n    def __init__(self, requires_grad=False, show_progress=False, use_relu=True):\n        super().__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True, progress=show_progress).features\n        if use_relu: \n            self.layer_names = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'conv4_2', 'relu5_1']\n            self.offset = 1\n        else:\n            self.layer_names = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv4_2', 'conv5_1']\n            self.offset = 0\n        self.contfeatmap_ind = 4  \n        \n        self.stylefeatmap_ind = list(range(len(self.layer_names)))\n        self.stylefeatmap_ind.remove(4)  \n\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.slice6 = torch.nn.Sequential()\n        for x in range(1+self.offset):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(1+self.offset, 6+self.offset):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(6+self.offset, 11+self.offset):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(11+self.offset, 20+self.offset):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(20+self.offset, 22):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(22, 29++self.offset):\n            self.slice6.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, x):\n        x = self.slice1(x)\n        layer1_1 = x\n        x = self.slice2(x)\n        layer2_1 = x\n        x = self.slice3(x)\n        layer3_1 = x\n        x = self.slice4(x)\n        layer4_1 = x\n        x = self.slice5(x)\n        conv4_2 = x\n        x = self.slice6(x)\n        layer5_1 = x\n        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n        out = vgg_outputs(layer1_1, layer2_1, layer3_1, layer4_1, conv4_2, layer5_1)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.732625Z","iopub.execute_input":"2025-01-12T16:22:06.732892Z","iopub.status.idle":"2025-01-12T16:22:06.745835Z","shell.execute_reply.started":"2025-01-12T16:22:06.732845Z","shell.execute_reply":"2025-01-12T16:22:06.745125Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def prepare_model(model, device):\n    model = Vgg19(requires_grad=False, show_progress=True)\n    \n    contfeatmap_ind = model.contfeatmap_ind\n    stylefeatmap_ind = model.stylefeatmap_ind\n    layer_names = model.layer_names\n\n    cont_indname = (contfeatmap_ind, layer_names[contfeatmap_ind])\n    style_fms_indices_names = (stylefeatmap_ind, layer_names)\n    return model.to(device).eval(), cont_indname, style_fms_indices_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.746811Z","iopub.execute_input":"2025-01-12T16:22:06.747043Z","iopub.status.idle":"2025-01-12T16:22:06.759612Z","shell.execute_reply.started":"2025-01-12T16:22:06.747021Z","shell.execute_reply":"2025-01-12T16:22:06.758910Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def gram_matrix(x, should_normalize=True):\n    (b, ch, h, w) = x.size()\n    features = x.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t)\n    if should_normalize:\n        gram /= ch * h * w\n    return gram\n\ndef total_variation(y):\n    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))\n\ndef compute_losses(model, img, target_rep, content_idx, style_indices, config):\n    target_content, target_styles = target_rep\n    features = model(img)\n\n    if isinstance(features, tuple):\n        features = list(features)\n\n    content_loss = torch.nn.functional.mse_loss(\n        target_content, features[content_idx].squeeze(0)\n    )\n\n    style_loss = sum(\n        torch.nn.functional.mse_loss(gram_matrix(features[idx]), gs)\n        for idx, gs in zip(style_indices, target_styles)\n    ) / len(target_styles)\n\n    tv_loss = total_variation(img)\n\n    total_loss = (\n        config['content_weight'] * content_loss +\n        config['style_weight'] * style_loss +\n        config['tv_weight'] * tv_loss\n    )\n\n    return total_loss, content_loss, style_loss, tv_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.762280Z","iopub.execute_input":"2025-01-12T16:22:06.762561Z","iopub.status.idle":"2025-01-12T16:22:06.770680Z","shell.execute_reply.started":"2025-01-12T16:22:06.762537Z","shell.execute_reply":"2025-01-12T16:22:06.770058Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def optimize_image(model, img, target_rep, content_idx, style_indices, config):\n    optimizer_type = config['optimizer']\n    num_iter = {'adam': 3000, 'lbfgs': 1000}[optimizer_type]\n\n    if optimizer_type == 'adam':\n        optimizer = Adam([img], lr=10)\n        for i in range(num_iter):\n            optimizer.zero_grad()\n            losses = compute_losses(\n                model, img, target_rep, content_idx, style_indices, config\n            )\n            losses[0].backward()\n            optimizer.step()\n            \n            # save_and_maybe_display(img, config['output_img_dir'], config, i, num_iter, False)\n            # print(f\"Adam | Iter {i+1}/{num_iter}: Loss={losses[0].item():.4f}\")\n            \n    elif optimizer_type == 'lbfgs':\n        optimizer = LBFGS([img], max_iter=num_iter, line_search_fn='strong_wolfe')\n        # iter_count = 0\n\n        def closure():\n            # nonlocal iter_count\n            optimizer.zero_grad()\n            losses = compute_losses(\n                model, img, target_rep, content_idx, style_indices, config\n            )\n            losses[0].backward()\n            # iter_count += 1\n            \n            # save_and_maybe_display(img, config['output_img_dir'], config, iter_count, num_iter, False)\n            # print(f\"L-BFGS | Iter {iter_count}/{num_iter}: Loss={losses[0].item():.4f}\")\n            return losses[0]\n\n        optimizer.step(closure)\n        \n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.771623Z","iopub.execute_input":"2025-01-12T16:22:06.771948Z","iopub.status.idle":"2025-01-12T16:22:06.784161Z","shell.execute_reply.started":"2025-01-12T16:22:06.771912Z","shell.execute_reply":"2025-01-12T16:22:06.783378Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def neural_style_transfer(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    content_img = prepare_img(config['content_img_name'], config['height'], device)\n    style_img = prepare_img(config['style_img_name'], config['height'], device)\n\n    model, cont_indname, style_fms_indices_names = prepare_model(config['model'], device)\n\n    content_features = model(content_img)[cont_indname[0]].squeeze(0).detach()\n    style_features = [\n        gram_matrix(model(style_img)[idx].detach())\n        for idx in style_fms_indices_names[0]\n    ]\n\n    opt_img = Variable(\n        content_img.clone(), requires_grad=True\n    ) if config['init_method'] == 'content' else Variable(\n        style_img.clone(), requires_grad=True\n    )\n\n    optimized_img = optimize_image(\n        model,\n        opt_img,\n        (content_features, style_features),\n        cont_indname[0],\n        style_fms_indices_names[0],\n        config\n    )\n    \n    final_img_path = os.path.join(config['output_img_dir'], generate_out_name(config))\n    save_image(optimized_img.squeeze(0).to('cpu').detach().numpy().transpose(1, 2, 0), final_img_path)\n    print(f\"Final image saved to {final_img_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.784966Z","iopub.execute_input":"2025-01-12T16:22:06.785174Z","iopub.status.idle":"2025-01-12T16:22:06.796982Z","shell.execute_reply.started":"2025-01-12T16:22:06.785152Z","shell.execute_reply":"2025-01-12T16:22:06.796099Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    config = {\n        'content_img_name': '/kaggle/input/neural-style-transfer-data/data/content-images/lion.jpg',\n        'style_img_name': '/kaggle/input/neural-style-transfer-data/data/style-images/vg_wheat_field.jpg',\n        'height': 512,  \n        'model': 'vgg19',  \n        'init_method': 'content',  \n        'optimizer': 'lbfgs',  \n        'content_weight': 1e5,\n        'style_weight': 1e8,\n        'tv_weight': 1e-4,\n        'output_img_dir': 'output/',  \n        'img_format': (5, '.png'),  \n        'savefreq': 50  \n    }\n    os.makedirs(config['output_img_dir'], exist_ok=True)\n    neural_style_transfer(config)\n    print('done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:22:06.797745Z","iopub.execute_input":"2025-01-12T16:22:06.798003Z","iopub.status.idle":"2025-01-12T16:23:10.461298Z","shell.execute_reply.started":"2025-01-12T16:22:06.797979Z","shell.execute_reply":"2025-01-12T16:23:10.460312Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 215MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Final image saved to output/lion_vg_wheat_field_o_lbfgs_i_content_h_512_m_vgg19_cw_100000.0_sw_100000000.0_tv_0.0001.png\ndone\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**Conclusion: \nNeural Style Transfer (NST) using VGG19 leverages the pre-trained convolutional layers of the network to extract and manipulate image features, effectively blending the content of one image with the artistic style of another. VGG19, known for its deep architecture and strong feature extraction capabilities, captures content information from deeper layers and style information from multiple shallower layers. By optimizing a loss function that balances content and style reconstruction, NST iteratively updates an input image to achieve the desired artistic transformation. Despite challenges like high computational costs and hyperparameter tuning, VGG19’s robust performance makes it a popular choice for NST, enabling creative applications in art and design.**","metadata":{}},{"cell_type":"code","source":"# import shutil\n\n# shutil.rmtree('/kaggle/working/output')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T16:23:10.462599Z","iopub.execute_input":"2025-01-12T16:23:10.463011Z","iopub.status.idle":"2025-01-12T16:23:10.467428Z","shell.execute_reply.started":"2025-01-12T16:23:10.462969Z","shell.execute_reply":"2025-01-12T16:23:10.466464Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}